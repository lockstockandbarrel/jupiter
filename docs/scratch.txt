[slurm-users] Large memory jobs stuck Pending. Should use --time parameter?
39 views
May 6, 2025, 11:08:17 AM
to slurm...@schedmd.com

Greetings,


We are new to Slurm and we are trying to better understand why we’re
seeing high-mem jobs stuck in Pending state indefinitely. Smaller (mem)
jobs in the queue will continue to pass by the high mem jobs even when we
bump priority on a pending high-mem job way up. We have been reading over
the backfill scheduling page and what we think we're seeing is that we
need to require that users specify a --time parameter on their jobs so
that Backfill works properly. None of our users specify a --time param
because we have never required it. Is that what we need to require
in order to fix this situation? From the backfill page:  "Backfill
scheduling is difficult without reasonable time limit estimates for
jobs, but some configuration parameters that can help" and it goes on
to list some config params that we have not set (DefaultTime, MaxTime,
OverTimeLimit). We also see language such as, “Since the expected
start time of pending jobs depends upon the expected completion time
of running jobs, reasonably accurate time limits are important for
backfill scheduling to work well.” So we suspect that we can achieve
proper backfill scheduling by requiring that all users supply a "--time"
parameter via a job submit plugin. Would that be a fair statement?

Thank you in advance!

-Mike Schor
May 6, 2025, 11:14:27 AM
to slurm...@lists.schedmd.com

Certainly it would help. Setting reasonable defaults for time is good
idea just in general. For instance we set 10 minutes as our default and
anything longer people have to explicitly request (up to the MaxTime
for the partition).

More to the point, what Reason does the scheduler give for what the job
is pending? If you do squeue or scontrol show job it should list the
reason why its pending. If its Resources, then the scheduler is waiting
for sufficient resources to free up to scheduler. If its is Priority
then the job is pending due to other jobs ahead of it.

-Paul Edmon-
May 6, 2025, 12:10:27 PM
to slurm...@lists.schedmd.com

Thank you so much for the prompt response! This makes a lot of sense. We
hadn't seen this explicitly stated in the docs, but it's what we gleaned
from them.

    -- 
    slurm-users mailing list -- slurm...@lists.schedmd.com
    To unsubscribe send an email to slurm-us...@lists.schedmd.com

May 7, 2025, 12:41:35 AM
to slurm...@lists.schedmd.com
Mike via slurm-users wrote:

> None of our users specify a --time param

If that results in a runtime of more than 28 days, the scheduler does
not reserve slots.

May 7, 2025, 3:13:26 AM
to slurm...@lists.schedmd.com
Mike via slurm-users
You might also need to look at the configuration parameter

SchedulerParameters

in particular

bf_window=#
The number of minutes into the future to look when considering jobs
to schedule. Higher values result in more overhead and less respon‐
siveness. A value at least as long as the highest allowed time limit
is generally advisable to prevent job starvation. In order to limit
the amount of data managed by the backfill scheduler, if the value of
bf_window is increased, then it is generally advisable to also
increase bf_resolution. This option applies only to Scheduler‐
Type=sched/backfill. Default: 1440 (1 day), Min: 1, Max: 43200 (30
days).

Regards

Loris Bennett

-- 
Dr. Loris Bennett (Herr/Mr)
FUB-IT, Freie Universität Berlin

AI Overview

The "unexpected missing socket error" in Slurm typically points to a
communication issue between Slurm components, particularly between the
slurmctld (controller) and slurmd (daemon on compute nodes), or within
the slurmctld itself when handling multiple connections. This error
often indicates a problem with network connectivity, resource exhaustion,
or configuration mismatches.
Possible causes and solutions:

    Network Connectivity Issues:

        Firewall restrictions: Ensure that the necessary ports for Slurm
        communication (e.g., SlurmctldPort, SlurmdPort) are open on all
        nodes and not blocked by firewalls.

        DNS problems: Verify that hostname resolution is working correctly
        between all Slurm components.

        Network instability: Investigate potential network issues, such
        as packet loss or high latency, which could lead to dropped
        connections.

    Resource Exhaustion:

        File descriptor limits: The slurmctld and slurmd processes might
        be hitting their limits for open file descriptors (sockets are
        treated as file descriptors).

            Solution: Increase LimitNOFILE in the slurmctld.service and
            slurmd.service systemd unit files, and potentially adjust
            the kernel's fs.file-max setting.

        RPC user limits: An overloaded slurmctld might struggle to handle
        the number of incoming RPCs.

            Solution: Consider optimizing SlurmctldParameters with
            settings like rl_enable, rl_bucket_size, rl_refill_period,
            rl_refill_rate, and rl_table_size to manage RPC limits.

    Configuration Mismatches:

        Slurm configuration: Ensure that slurm.conf is consistent across
        all nodes and that SlurmctldPort and SlurmdPort are correctly
        configured.

        Time synchronization: Discrepancies in system time between
        nodes can cause issues with Munge authentication and Slurm
        communication.

            Solution: Use NTP or similar services to synchronize time
            across all nodes.

    SSSD Configuration:

        SSSD performance: If SSSD is used for authentication, its
        performance can impact Slurm.

            Solution: Optimize SSSD configuration, potentially by moving
            the SSSD cache to tmpfs or adjusting ACI settings on the
            server side.

    Slurm Message Timeout:

        Solution: Increase the MessageTimeout parameter in slurm.conf if
        communication delays are suspected, especially in environments
        with shared nodes where slurmd might be paged out.

Troubleshooting Steps:

    Check Slurm logs: Examine slurmctld.log and slurmd.log on all relevant
    nodes for more specific error messages that might pinpoint the cause.

    Monitor system resources: Track CPU usage, memory, network activity,
    and file descriptor usage on controller and compute nodes, especially
    around the time the error occurs.

    Verify network connectivity: Use tools like ping, traceroute,
    and netcat to test connectivity between Slurm components on the
    configured ports.

    Review slurm.conf: Ensure consistency and correctness of all relevant
    parameters.

    Test Munge: Verify Munge functionality, as it's crucial for Slurm
    authentication.

Addressing these potential causes systematically will help diagnose and
resolve the "unexpected missing socket error."


slurm-dev@schedmd.com
Discussion:
Slurm errors out when a burst of jobs are submitted
Paul Thirumalai
15 years ago
Permalink
Hi
I have about 350 single cpu machines in my cluster. I need to run ~10K jobs
on these machines. I am submitting the job using the followign sbatch
command. Each job takes about 90 seconds to complete.

I run /usr/bin/sbatch --begin=now <jobscript> <parameters>

It seems that the first few thousand jobs are launched correctly. But once
in a while I see the following errors. (For about 300 jobs out of 10K)

srun: error: slurm_receive_msg: Socket timed out on send/recv operation
srun: error: Unable to confirm allocation for job 55286: Socket timed out on
send/recv operation
srun: Check SLURM_JOB_ID environment variable for expired or invalid job.

When i launch about 3000 jobs I dont see these errors. This would lead me to
beleive that the large volume of jobs is what is causing these errors. What
I have tried to do to alleviate this issue is create a batch size, i.e
submit 500 jobs at a time and sleep for 5 seconds, submit the next 500 jobs
and so on. This does not really seem to help.

Is there some way I could avoid these errors. Any help appreciated.
Danny Auble
15 years ago
Permalink
Hey Paul,

Are you saying you are starting 1 job with 10k steps, or 10k sbatchs?

If 1 sbatch with 10k sruns you should probably up your MessageTimeout
to 30 or 60, the problem here is your slurmctld is probably getting
overloaded. What is your debug level set to in the slurmctld log? If
you want better performance you should make it smaller.

If you are starting 10k sbatchs you should follow some of the ideas on this page...

https://computing.llnl.gov/linux/slurm/high_throughput.html

In particular,

*SchedulerParameters=defer*

Danny
...
Paul Thirumalai
15 years ago
Permalink
its 10K sbatches. I am going to play with the suggestions here and report
back.
...
Paul Thirumalai
15 years ago
Permalink
I set SchedulerParameters=max_job_bf=15,interval=20, but the problem
persists.

I am going to see if setting SHARED=EXCLUSIVE will fix this problem
Paul Thirumalai
15 years ago
Permalink
So I made the following changes to slurm.conf.

SchedulerParameters=max_job_bf=100,interval=30
MinJobAge=20
MessageTimeout=30
MaxJobCount=50000

I also changed the parameter
SlurmctldPort=6820-6823

This way slurmctld takes request on mutipel nodes.

Now when I launch jobs the jobs never run, they are always in Pending state.
The contents of slurm.conf are below. Thanks in advance


# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=z21
#ControlAddr=
BackupController=z18
#BackupAddr=
#
AuthType=auth/none
#AuthType=auth/munge
CacheGroups=0
#CheckpointType=checkpoint/none
CryptoType=crypto/munge
#DisableRootJobs=NO
#EnforcePartLimits=NO
#Epilog=
#PrologSlurmctld=
#FirstJobId=1
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobCheckpointDir=/var/slurm/checkpoint
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=1
#KillOnBadExit=0
#Licenses=foo*4,bar
#MailProg=/bin/mail
MaxJobCount=50000
#MaxTasksPerNode=128
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
#ProctrackType=proctrack/pgid
ProctrackType=proctrack/linuxproc

#Prolog=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
ReturnToService=1
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817-6820
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6821
SlurmdSpoolDir=/tmp/slurmd
SlurmUser=slurm
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/tmp
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/none
#TaskPluginParam=
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFs=/tmp
#TrackWCKey=no
#TreeWidth=327
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
MessageTimeout=30
#ResvOverRun=0
MinJobAge=20
#OverTimeLimit=0
SlurmctldTimeout=120

SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=0
FastSchedule=1
#MaxMemPerCPU=0
#SchedulerRootFilter=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerPort=7321
SchedulerParameters=max_job_bf=100,interval=30
SelectType=select/linear
#SelectType=select/cons_res
#SelectTypeParameters=CR_CPU
#
#
# JOB PRIORITY
#PriorityType=priority/basic
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
#PriorityWeightPartition=
#PriorityWeightQOS=
#
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
AccountingStorageHost=z21
AccountingStorageLoc=slurm_job_acc
AccountingStoragePass=slurm
AccountingStoragePort=3306
AccountingStorageType=accounting_storage/mysql
AccountingStorageUser=slurm
ClusterName=cluster
#DebugFlags=
JobCompHost=z21
JobCompLoc=slurm_job_comp
JobCompPass=slurm
JobCompPort=3306
JobCompType=jobcomp/mysql
JobCompUser=slurm
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=7
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=7
SlurmdLogFile=/var/log/slurm/slurmd.log.%h
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
NodeName=z[23,24,26,28-39] Procs=4 State=UNKNOWN
NodeName=z[25,27] Procs=2 State=UNKNOWN
NodeName=obi[23-40] Procs=4 State=UNKNOWN
NodeName=w[001-108] Procs=2 State=UNKNOWN
NodeName=y[002-010,012-025,027-038] Procs=2 State=UNKNOWN
NodeName=y[040-062,064-108,111-119,121-186] Procs=4 State=UNKNOWN
PartitionName=z_part Nodes=z[23-39] Default=NO Shared=NO MaxTime=INFINITE
State=UP
#PartitionName=obi_part Nodes=obi[23-40] Default=NO Shared=NO
MaxTime=INFINITE State=UP
PartitionName=w_part Nodes=w[001-108] Default=NO Shared=NO MaxTime=INFINITE
State=UP
PartitionName=y_part
Nodes=y[002-010,012-025,027-038,040-062,064-108,111-119,121-186] Default=NO
Shared=NO MaxTime=INFINITE State=UP
#PartitionName=all_part
Nodes=w[001-108],y[2-10,12-25,27-38,41-62,64-108,111-119,121-186]
Default=YES MaxTime=INFINITE State=UP
#PartitionName=all_part
Nodes=z[23-39,41-46],obi[23-40],w[1-108],y[2-10,12-25,27-38,41-62,64-108,111-119,121-186]
Shared=NO Default=YES MaxTime=INFINITE State=UP
PartitionName=all_part
Nodes=z[23-39],w[001-108],y[002-010,012-025,027-038,040-062,064-108,111-119,121-186]
Shared=NO Default=YES MaxTime=INFINITE State=UP
paran-5UnqSh4Icw/LoDKTGw+ (Pär Andersson)
15 years ago
Permalink
Paul,
Post by Paul Thirumalai
So I made the following changes to slurm.conf.
...
Post by Paul Thirumalai
MessageTimeout=30
Do you notice any difference if you set MessageTimeout to something
really high, like 300 or so?

Do you get any error messages in your /var/log/slurm/slurmctld.log?

Have you tried the suggestion of lowering the debug level? In your
slurm.conf you have SlurmctldDebug=7 wich will result in lots of debug
messages in slurmctld.log wich might slow the slurmctld down.

Regards,

Pär Andersson
NSC
Paul Thirumalai
15 years ago
Permalink
Thanks Par,
In the slurmctld.log file I see the following error all over the place

Error connecting slurm stream socket at 192.168.1.18:6820: Connection
refused

6820 is one of my slurmctld ports

I will try your suggestions and see if it helps
Paul Thirumalai
15 years ago
Permalink
When I change MessageTimeout=300 , I get the following error when I start
slurmd
rsh y119 "/etc/init.d/emunge start; /etc/init.d/slurm start &"
Starting MUNGE: munged[ OK ]
scontrol: WARNING: MessageTimeout is too high for effective fault-tolerance
starting slurmd: slurmd: WARNING: MessageTimeout is too high for effective
fault-tolerance

Slurm is still running though. I will play decreasing this value to see if
it helps.

Thanks for you help
Paul Thirumalai
15 years ago
Permalink
Good news: Seems like changing the MessageTimeout to 300 has resolved the
issue. This is the only change I made in addition to SchedulerParameters
change and SlurmctldPorts change which is listed above. I rolled back all
the other changes

The bad news is that my log file has thousands of messages that read
srun: WARNING: MessageTimeout is too high for effective fault-tolerance
and
sbatch: WARNING: MessageTimeout is too high for effective fault-tolerance

I will have to play with the value of MessageTimeout to see if I can get rid
of this error message.
Matthieu Hautreux
15 years ago
Permalink
Hi it seems that your backup controller does not use the new configuration
file and so is not listening on the 6820 port. This message seems to come
from unsuccessful attempts to ask the backup controller to stop acting as
the primary.

Otherwise, how long are your jobs running ? Is it short jobs or long jobs ?
sometimes, the controller can be slowdown by massive amount of jobs that
ends at the same time.

HTH
Matthieu
Post by Paul Thirumalai
Thanks Par,
In the slurmctld.log file I see the following error all over the place
Error connecting slurm stream socket at 192.168.1.18:6820: Connection
refused
6820 is one of my slurmctld ports
I will try your suggestions and see if it helps
Paul Thirumalai
15 years ago

Yes you're right. The backup controller does not have the update slurm.conf.

The jobs run for anywhere from 30 -120 seconds. And yes alot of the jobs
actually seem to end at right around the same time.

